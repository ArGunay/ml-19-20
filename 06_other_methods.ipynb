{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_other_methods.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreacini/ml-19-20/blob/master/06_other_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu4HJeN686y2",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning\n",
        "\n",
        "Prof. Cesare Alippi  \n",
        "Andrea Cini  ([`andrea.cini@usi.ch`](mailto:daniele.grattarola@usi.ch)  )    \n",
        "Daniele Zambon ([`daniele.zambon@usi.ch`](mailto:daniele.zambon@usi.ch)  )\n",
        "\n",
        "---\n",
        "# Lab 06: Other methods\n",
        "\n",
        "In this lab we will see how to use some of the more advanced methods that we saw in the last lectures.\n",
        "\n",
        "\n",
        "---\n",
        "# Trees\n",
        "\n",
        "Let's start defining our usual helper functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IgAvdbC1482",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first we define some helper functions to generate data and plot results\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification, make_circles, make_moons\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "\n",
        "# function to generate classification problems\n",
        "def get_data(n, ctype='simple'):\n",
        "  if ctype == 'simple':\n",
        "    x, y = make_classification(n_features=2, \n",
        "                               n_redundant=0, \n",
        "                               n_informative=2, \n",
        "                               n_clusters_per_class=1)\n",
        "    x += np.random.uniform(size=x.shape) # add some noise\n",
        "  elif ctype == 'circles':\n",
        "    x, y = make_circles(n, noise=0.1, factor=0.5)\n",
        "  \n",
        "  elif ctype == 'moons':\n",
        "    x, y = make_moons(n, noise=0.1)\n",
        "  else:\n",
        "    raise ValueError\n",
        "  return x, y\n",
        "\n",
        "# function to plot decision boundaries\n",
        "def plot_decision_surface(model, x, y, transform=lambda x:x):    \n",
        "  #init figure\n",
        "  fig = plt.figure()\n",
        "\n",
        "  # Create mesh\n",
        "  h = .01  # step size in the mesh\n",
        "  x_min, x_max = x[:, 0].min() - .5, x[:, 0].max() + .5\n",
        "  y_min, y_max = x[:, 1].min() - .5, x[:, 1].max() + .5\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                        np.arange(y_min, y_max, h))\n",
        "\n",
        "  # plot train data\n",
        "  plt.scatter(x[:, 0], x[:, 1], c=y, edgecolors='k')\n",
        "\n",
        "  plt.xlim(xx.min(), xx.max())\n",
        "  plt.ylim(yy.min(), yy.max())\n",
        "\n",
        "  plt.xlabel(r'$x_1$')\n",
        "  plt.ylabel(r'$x_2$');\n",
        "\n",
        "  y_pred = model.predict(transform(np.c_[xx.ravel(), yy.ravel()]))\n",
        "\n",
        "  y_pred = y_pred.reshape(xx.shape)\n",
        "  plt.contourf(xx, yy, y_pred, alpha=.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X6JtuoMlqCq",
        "colab_type": "text"
      },
      "source": [
        "We are going to use the [Iris](https://archive.ics.uci.edu/ml/datasets/iris) dataset where the objective is to classify flowers based on some features:\n",
        "\n",
        "1. sepal length in cm\n",
        "2. sepal width in cm\n",
        "3. petal length in cm\n",
        "4. petal width in cm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn7xqfdkmfCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        " \n",
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# import pandas as pd\n",
        "# data = pd.DataFrame(np.c_[iris.data, iris.target], columns=iris.feature_names + ['Class',])\n",
        "\n",
        "# data.sample(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvLg7L-imfjV",
        "colab_type": "text"
      },
      "source": [
        "Let's give a look at the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SjvNW28BxGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_prime = x[:, :2]\n",
        "\n",
        "plt.scatter(x_prime[:, 0], x_prime[:, 1], c=y)\n",
        "plt.xlabel('sepal length (cm)')\n",
        "plt.ylabel('sepal width (cm)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbIvjOQ49Udr",
        "colab_type": "text"
      },
      "source": [
        "Again we can use `scikitlearn` to build Decision Trees preatty easily in python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayq8nv8xojjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "classifier = DecisionTreeClassifier() # create an instance of the model\n",
        "classifier.fit(x_prime, y)            # fit the data\n",
        "\n",
        "plot_decision_surface(classifier, x_prime, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reMC63iSnWPJ",
        "colab_type": "text"
      },
      "source": [
        "Watch out for overfitting!\n",
        "\n",
        "Now let's try to build a tree using all the features. To visualize the result we'll look directly at the tree.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7hQaTYdqx5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import plot_tree\n",
        "\n",
        "classifier = DecisionTreeClassifier(criterion='entropy')\n",
        "classifier.fit(x, y)  \n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "plot_tree(classifier, filled=True, feature_names=iris.feature_names, rounded=True, class_names=iris.target_names);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8oD-KbOnpGj",
        "colab_type": "text"
      },
      "source": [
        "Decision Trees are easy to interpret and that's way they are really popular in financial applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fle838Qjy4I9",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machines\n",
        "\n",
        "Let's try to use Support Vector Machines to solve our problem, and check how the results change using different kernels.\n",
        "\n",
        "Remeber, a kernel (oversimplifying a lot) is a function that gives us a particular measure of affinity between two points. We can use kernels in the dual formulation of the SVM problem to project the input space in an high (possibly infinite) dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi0MniB74Q7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "classifier = SVC(kernel='linear')\n",
        "\n",
        "classifier.fit(x_prime, y)\n",
        "\n",
        "plot_decision_surface(classifier, x_prime, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVwVoxFoEi1F",
        "colab_type": "text"
      },
      "source": [
        "Tuning correctly the hyperparameters is foundamental (check [here]()).\n",
        "\n",
        "Let's use the XOR problem to have a better intuation fo the decision boundaries found by SVMs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rlSbxu8_jM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(10)\n",
        "\n",
        "x = np.random.randn(150, 2) # sample some points from a bivariate diagonal gaussian\n",
        "y = np.logical_xor(x[:, 0] > 0., x[:, 1] > 0.)\n",
        "\n",
        "plt.scatter(x[:, 0], x[:, 1], c=y)\n",
        "plt.xlabel(r'$x_1$')\n",
        "plt.ylabel(r'$x_2$')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhzsyXIi94NU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "classifier = SVC(kernel='rbf')\n",
        "\n",
        "classifier.fit(x, y)\n",
        "\n",
        "plot_decision_surface(classifier, x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "977N-SIEOxMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}